{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Data sheet is at\n",
    "https://docs.google.com/spreadsheets/d/1NCoQiCwjPM42AQRq1qj6UQFv8EKwUeK24eiQ1IChe3I/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    User  Book 1  Book 2  Book 3  Book 4  Book 5  Book 6\n",
      "0  User1     5.0     2.0     NaN     NaN     NaN     NaN\n",
      "1  User2     NaN     5.0     NaN     1.0     3.0     2.0\n",
      "2  User3     NaN     NaN     4.0     NaN     NaN     NaN\n",
      "3  User4     3.0     1.0     NaN     3.0     NaN     4.0\n",
      "4  User5     2.0     NaN     1.0     NaN     5.0     2.0\n",
      "5  User6     1.0     2.0     1.0     NaN     5.0     1.0\n",
      "6  User7     NaN     1.0     1.0     1.0     NaN     1.0\n",
      "7  User8     NaN     NaN     NaN     1.0     NaN     NaN\n",
      "8  User9     1.0     NaN     5.0     NaN     2.0     1.0\n",
      "   user_id book_id  rating\n",
      "0    User1  Book 1     1.0\n",
      "3    User4  Book 1     0.6\n",
      "4    User5  Book 1     0.4\n",
      "5    User6  Book 1     0.2\n",
      "8    User9  Book 1     0.2\n",
      "9    User1  Book 2     0.4\n",
      "10   User2  Book 2     1.0\n",
      "12   User4  Book 2     0.2\n",
      "14   User6  Book 2     0.4\n",
      "15   User7  Book 2     0.2\n",
      "20   User3  Book 3     0.8\n",
      "22   User5  Book 3     0.2\n",
      "23   User6  Book 3     0.2\n",
      "24   User7  Book 3     0.2\n",
      "26   User9  Book 3     1.0\n",
      "28   User2  Book 4     0.2\n",
      "30   User4  Book 4     0.6\n",
      "33   User7  Book 4     0.2\n",
      "34   User8  Book 4     0.2\n",
      "37   User2  Book 5     0.6\n",
      "40   User5  Book 5     1.0\n",
      "41   User6  Book 5     1.0\n",
      "44   User9  Book 5     0.4\n",
      "46   User2  Book 6     0.4\n",
      "48   User4  Book 6     0.8\n",
      "49   User5  Book 6     0.4\n",
      "50   User6  Book 6     0.2\n",
      "51   User7  Book 6     0.2\n",
      "53   User9  Book 6     0.2\n"
     ]
    }
   ],
   "source": [
    "# convert into data from matrix to long format\n",
    "import pandas as pd\n",
    "# set pandas to show all columns without truncation and line breaks\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# data = np.loadtxt('data/test-data.csv', delimiter=',', dtype=int, skiprows=1,)\n",
    "data = pd.read_csv('data/test-data.csv')\n",
    "print(data)\n",
    "\n",
    "# reset the column.index to be numeric\n",
    "user_index = data[data.columns[0]]\n",
    "book_index = data.columns\n",
    "data = data.reset_index(drop=True)\n",
    "data[data.columns[0]] = data.index.astype('int')\n",
    "# print(data)\n",
    "# print(data)\n",
    "scaler = 5\n",
    "\n",
    "# data = pd.DataFrame(data.to_numpy(), index=range(0,len(user_index)), columns=range(0,len(book_index)))\n",
    "df_long = pd.melt(data, id_vars=[data.columns[0]], \n",
    "                  ignore_index=True, \n",
    "                  var_name='book_id', \n",
    "                  value_name='rate').dropna()\n",
    "df_long.columns = ['user_id', 'book_id', 'rating']\n",
    "df_long['rating'] = df_long['rating'] / scaler\n",
    "# replace the user_id to user by match user_index\n",
    "df_long['user_id'] = df_long['user_id'].apply(lambda x: user_index[x])\n",
    "# data = df_long.to_numpy()\n",
    "\n",
    "print(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 09:42:26.734174: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 09:42:27.422544: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-20 09:42:29.571778: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-20 09:42:29.571916: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-20 09:42:29.571924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-20 09:42:34.835852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:35.000334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:35.001700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:35.004716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 09:42:35.006738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:35.008048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:35.009285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:36.889905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:36.893055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:36.894304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 09:42:36.895527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13641 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the MovieLens dataset\n",
    "# url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "# dataset = pd.read_csv(url, compression='zip', usecols=['userId', 'movieId', 'rating'])\n",
    "dataset = df_long\n",
    "# Encode the user and movie IDs\n",
    "user_encoder = LabelEncoder()\n",
    "book_encoder = LabelEncoder()\n",
    "dataset['user_id'] = user_encoder.fit_transform(dataset['user_id'])\n",
    "dataset['book_id'] = book_encoder.fit_transform(dataset['book_id'])\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "# train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train = dataset\n",
    "\n",
    "# Model hyperparameters\n",
    "num_users = len(dataset['user_id'].unique())\n",
    "num_countries = len(dataset['book_id'].unique())\n",
    "embedding_dim = 64\n",
    "\n",
    "# Create the NCF model\n",
    "inputs_user = tf.keras.layers.Input(shape=(1,))\n",
    "inputs_book = tf.keras.layers.Input(shape=(1,))\n",
    "embedding_user = tf.keras.layers.Embedding(num_users, embedding_dim)(inputs_user)\n",
    "embedding_book = tf.keras.layers.Embedding(num_countries, embedding_dim)(inputs_book)\n",
    "\n",
    "# Merge the embeddings using concatenation, you can also try other merging methods like dot product or multiplication\n",
    "merged = tf.keras.layers.Concatenate()([embedding_user, embedding_book])\n",
    "merged = tf.keras.layers.Flatten()(merged)\n",
    "\n",
    "# Add fully connected layers\n",
    "dense = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "dense = tf.keras.layers.Dense(32, activation='relu')(dense)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "# Compile the model\n",
    "model = tf.keras.Model(inputs=[inputs_user, inputs_book], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 09:42:40.739477: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55ed08429630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-20 09:42:40.739512: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-03-20 09:42:40.803753: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-20 09:42:41.356687: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ca8254dc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [train['user_id'].values, train['book_id'].values],\n",
    "    train['rating'].values,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    # validation_split=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8020/804887845.py:15: FutureWarning: this method is deprecated in favour of `Styler.to_html()`\n",
      "  display(HTML(out.render()))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_63063\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_63063_level0_col0\" class=\"col_heading level0 col0\" >Book 1</th>\n",
       "      <th id=\"T_63063_level0_col1\" class=\"col_heading level0 col1\" >Book 2</th>\n",
       "      <th id=\"T_63063_level0_col2\" class=\"col_heading level0 col2\" >Book 3</th>\n",
       "      <th id=\"T_63063_level0_col3\" class=\"col_heading level0 col3\" >Book 4</th>\n",
       "      <th id=\"T_63063_level0_col4\" class=\"col_heading level0 col4\" >Book 5</th>\n",
       "      <th id=\"T_63063_level0_col5\" class=\"col_heading level0 col5\" >Book 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row0\" class=\"row_heading level0 row0\" >User1</th>\n",
       "      <td id=\"T_63063_row0_col0\" class=\"data row0 col0\" >4.76</td>\n",
       "      <td id=\"T_63063_row0_col1\" class=\"data row0 col1\" >2.04</td>\n",
       "      <td id=\"T_63063_row0_col2\" class=\"data row0 col2\" >4.68</td>\n",
       "      <td id=\"T_63063_row0_col3\" class=\"data row0 col3\" >3.71</td>\n",
       "      <td id=\"T_63063_row0_col4\" class=\"data row0 col4\" >4.24</td>\n",
       "      <td id=\"T_63063_row0_col5\" class=\"data row0 col5\" >4.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row1\" class=\"row_heading level0 row1\" >User2</th>\n",
       "      <td id=\"T_63063_row1_col0\" class=\"data row1 col0\" >2.48</td>\n",
       "      <td id=\"T_63063_row1_col1\" class=\"data row1 col1\" >4.67</td>\n",
       "      <td id=\"T_63063_row1_col2\" class=\"data row1 col2\" >4.13</td>\n",
       "      <td id=\"T_63063_row1_col3\" class=\"data row1 col3\" >1.11</td>\n",
       "      <td id=\"T_63063_row1_col4\" class=\"data row1 col4\" >3.09</td>\n",
       "      <td id=\"T_63063_row1_col5\" class=\"data row1 col5\" >1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row2\" class=\"row_heading level0 row2\" >User3</th>\n",
       "      <td id=\"T_63063_row2_col0\" class=\"data row2 col0\" >2.16</td>\n",
       "      <td id=\"T_63063_row2_col1\" class=\"data row2 col1\" >3.67</td>\n",
       "      <td id=\"T_63063_row2_col2\" class=\"data row2 col2\" >4.00</td>\n",
       "      <td id=\"T_63063_row2_col3\" class=\"data row2 col3\" >1.65</td>\n",
       "      <td id=\"T_63063_row2_col4\" class=\"data row2 col4\" >4.08</td>\n",
       "      <td id=\"T_63063_row2_col5\" class=\"data row2 col5\" >2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row3\" class=\"row_heading level0 row3\" >User4</th>\n",
       "      <td id=\"T_63063_row3_col0\" class=\"data row3 col0\" >3.04</td>\n",
       "      <td id=\"T_63063_row3_col1\" class=\"data row3 col1\" >0.93</td>\n",
       "      <td id=\"T_63063_row3_col2\" class=\"data row3 col2\" >4.49</td>\n",
       "      <td id=\"T_63063_row3_col3\" class=\"data row3 col3\" >3.02</td>\n",
       "      <td id=\"T_63063_row3_col4\" class=\"data row3 col4\" >3.38</td>\n",
       "      <td id=\"T_63063_row3_col5\" class=\"data row3 col5\" >3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row4\" class=\"row_heading level0 row4\" >User5</th>\n",
       "      <td id=\"T_63063_row4_col0\" class=\"data row4 col0\" >2.01</td>\n",
       "      <td id=\"T_63063_row4_col1\" class=\"data row4 col1\" >2.86</td>\n",
       "      <td id=\"T_63063_row4_col2\" class=\"data row4 col2\" >1.02</td>\n",
       "      <td id=\"T_63063_row4_col3\" class=\"data row4 col3\" >1.32</td>\n",
       "      <td id=\"T_63063_row4_col4\" class=\"data row4 col4\" >4.75</td>\n",
       "      <td id=\"T_63063_row4_col5\" class=\"data row4 col5\" >1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row5\" class=\"row_heading level0 row5\" >User6</th>\n",
       "      <td id=\"T_63063_row5_col0\" class=\"data row5 col0\" >0.99</td>\n",
       "      <td id=\"T_63063_row5_col1\" class=\"data row5 col1\" >2.04</td>\n",
       "      <td id=\"T_63063_row5_col2\" class=\"data row5 col2\" >0.95</td>\n",
       "      <td id=\"T_63063_row5_col3\" class=\"data row5 col3\" >0.78</td>\n",
       "      <td id=\"T_63063_row5_col4\" class=\"data row5 col4\" >4.66</td>\n",
       "      <td id=\"T_63063_row5_col5\" class=\"data row5 col5\" >1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row6\" class=\"row_heading level0 row6\" >User7</th>\n",
       "      <td id=\"T_63063_row6_col0\" class=\"data row6 col0\" >0.94</td>\n",
       "      <td id=\"T_63063_row6_col1\" class=\"data row6 col1\" >1.01</td>\n",
       "      <td id=\"T_63063_row6_col2\" class=\"data row6 col2\" >1.08</td>\n",
       "      <td id=\"T_63063_row6_col3\" class=\"data row6 col3\" >0.75</td>\n",
       "      <td id=\"T_63063_row6_col4\" class=\"data row6 col4\" >3.89</td>\n",
       "      <td id=\"T_63063_row6_col5\" class=\"data row6 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row7\" class=\"row_heading level0 row7\" >User8</th>\n",
       "      <td id=\"T_63063_row7_col0\" class=\"data row7 col0\" >1.89</td>\n",
       "      <td id=\"T_63063_row7_col1\" class=\"data row7 col1\" >1.86</td>\n",
       "      <td id=\"T_63063_row7_col2\" class=\"data row7 col2\" >2.67</td>\n",
       "      <td id=\"T_63063_row7_col3\" class=\"data row7 col3\" >1.06</td>\n",
       "      <td id=\"T_63063_row7_col4\" class=\"data row7 col4\" >3.92</td>\n",
       "      <td id=\"T_63063_row7_col5\" class=\"data row7 col5\" >1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63063_level0_row8\" class=\"row_heading level0 row8\" >User9</th>\n",
       "      <td id=\"T_63063_row8_col0\" class=\"data row8 col0\" >0.90</td>\n",
       "      <td id=\"T_63063_row8_col1\" class=\"data row8 col1\" >1.89</td>\n",
       "      <td id=\"T_63063_row8_col2\" class=\"data row8 col2\" >4.68</td>\n",
       "      <td id=\"T_63063_row8_col3\" class=\"data row8 col3\" >1.16</td>\n",
       "      <td id=\"T_63063_row8_col4\" class=\"data row8 col4\" >1.86</td>\n",
       "      <td id=\"T_63063_row8_col5\" class=\"data row8 col5\" >1.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "result_df = {}\n",
    "for user_i in range(1, 10):\n",
    "  user = f'User{user_i}'\n",
    "  result_df[user] = {}\n",
    "  for book_i in range(1, 7):    \n",
    "    book = f'Book {book_i}'\n",
    "    pred_user_id = user_encoder.transform([user])\n",
    "    pred_book_id = book_encoder.transform([book])\n",
    "    result = model.predict(x=[pred_user_id, pred_book_id], verbose=0)\n",
    "    result_df[user][book] = result[0][0]\n",
    "result_df = pd.DataFrame(result_df).T\n",
    "result_df *= scaler\n",
    "out = result_df.style.format(\"{:.2f}\")\n",
    "display(HTML(out.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-cf-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
